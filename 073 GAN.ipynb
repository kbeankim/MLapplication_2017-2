{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Box office \n",
    "\n",
    "[List of all named GANs](https://deephunt.in/the-gan-zoo-79597dc8c347) \n",
    "\n",
    "[Fantastic GANs and where to find them](http://guimperarnau.com/blog/2017/03/Fantastic-GANs-and-where-to-find-them)\n",
    "\n",
    "[Delving deep into GANs](http://gkalliatakis.com/blog/delving-deep-into-gans)\n",
    "\n",
    "Tensorflow Generative Model Collections [이활석](https://github.com/hwalsuklee/tensorflow-generative-model-collections)\n",
    "\n",
    "Generative Adversarial Nets in TensorFlow [wiseodd](http://wiseodd.github.io/techblog/2016/09/17/gan-tensorflow/) [code](https://github.com/wiseodd/generative-models/blob/master/GAN/vanilla_gan/gan_tensorflow.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN vs VAE\n",
    "\n",
    "<img src=\"img/GAN-GAN_vs_VAE.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [GAN](https://arxiv.org/pdf/1406.2661.pdf) [local-paper](http://localhost:8888/notebooks/Dropbox/Paper/1406-2661.pdf) [code-goodfeli](https://github.com/goodfeli/adversarial)  [slide](https://www.slideshare.net/ssuser77ee21/generative-adversarial-networks-70896091?from_action=save) [slide](http://www.cs.toronto.edu/~dtarlow/pos14/talks/goodfellow.pdf) [slide](https://www.slideshare.net/ThomasDaSilvaPaula/a-very-gentle-introduction-to-generative-adversarial-networks-aka-gans-71614428) [demo](http://cs.stanford.edu/people/karpathy/gan/) [talk1](https://www.youtube.com/watch?v=AJVyzd0rqdc) [talk1 - material](https://arxiv.org/abs/1701.00160) [talk2](https://www.youtube.com/watch?v=HN9NRhm9waY&list=PL9hdKtbjIQ0wrszVsj3SWV-V2OAnMXZWa&index=5)\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"img/GAN_train 2.png\"/>\n",
    "\n",
    "Generating faces by \n",
    "[VAE/GAN](https://www.youtube.com/watch?v=X3mULgQ-G0A) \n",
    "[conv/deconv GAN](https://www.youtube.com/watch?v=QYcKCGdZvq4) \n",
    "[DCGAN](https://www.youtube.com/watch?v=Svk0SxyNdr8) \n",
    "[WGAN 1](https://www.youtube.com/watch?v=kDJXG2D4_Ck) \n",
    "[WGAN 2](https://www.youtube.com/watch?v=8Rxgzjdk5TA&t=3s) \n",
    "\n",
    "<img src=\"img/GAN.png\"/>\n",
    "\n",
    "An introduction to Generative Adversarial Networks (with code in TensorFlow) [video](https://www.youtube.com/watch?v=mObnwR-u8pc) [blog](http://blog.aylien.com/introduction-generative-adversarial-networks-code-tensorflow/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/GAN - minmax problem.png\"/>\n",
    "\n",
    "<img src=\"img/GAN - minmax problem 2.png\"/>\n",
    "\n",
    "<img src=\"img/GAN - minmax problem 3.png\"/>\n",
    "\n",
    "<img src=\"img/GAN - minmax problem 4.png\"/>\n",
    "\n",
    "<img src=\"img/GAN - Prop 1.png\"/>\n",
    "\n",
    "<img src=\"img/GAN - Thm 1.png\"/>\n",
    "\n",
    "<img src=\"img/GAN - algorithm.png\"/>\n",
    "\n",
    "<img src=\"img/GAN - Thm 2.png\"/>\n",
    "\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "x&\\quad\\longleftrightarrow\\quad\\quad&p_g\\\\\n",
    "\\alpha&\\quad\\longleftrightarrow\\quad\\quad&D\\\\\n",
    "f_{\\alpha}(x)&\\quad\\longleftrightarrow\\quad\\quad&U(p_g,D)\\\\\n",
    "\\sup_\\alpha f_{\\alpha}(x)&\\quad\\longleftrightarrow\\quad\\quad&\\sup_D U(p_g,D)\\\\\n",
    "f_{\\alpha}(x)\\ \\mbox{convex in $x$}\\ \\Rightarrow\\  \\sup_\\alpha f_{\\alpha}(x)\\ \\mbox{convex in $x$}&\\quad\\longleftrightarrow\\quad\\quad&U(p_g,D)\\ \\mbox{convex in $p_g$}\\ \\Rightarrow\\  \\sup_D U(p_g,D)\\ \\mbox{convex in $p_g$}\\\\\n",
    "\\end{eqnarray*}\n",
    "\n",
    "<img src=\"img/GAN - Thm 2 - subderivative.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PR-001: Generative adversarial nets\n",
    "[유재준](https://www.youtube.com/watch?v=L3hz57whyNw&t=3s)\n",
    "[blog 1](http://jaejunyoo.blogspot.com/2017/01/generative-adversarial-nets-1.html)\n",
    "[blog 2](http://jaejunyoo.blogspot.com/2017/01/generative-adversarial-nets-2.html)\n",
    "[slide](https://www.slideshare.net/thinkingfactory/pr12-intro-to-gans-jaejun-yoo)\n",
    "[local-slide](http://localhost:8888/notebooks/Dropbox/Paper/PR12와 함께 이해하는 GANS.pdf)\n",
    "\n",
    "\n",
    "Generative adversarial networks\n",
    "[김남주](https://www.slideshare.net/ssuser77ee21/generative-adversarial-networks-70896091)\n",
    "[local-slide](http://localhost:8888/notebooks/Dropbox/Paper/generativeadversarialnetworks-김남주.pdf)\n",
    "\n",
    "텐서플로우 설치도 했고 튜토리얼도 봤고 기초 예제도 짜봤다면 TensorFlow KR Meetup 2016\n",
    "[김태훈](https://www.slideshare.net/NaverEngineering/1-gangenerative-adversarial-network)\n",
    "[local-slide](http://localhost:8888/notebooks/Dropbox/Paper/p2-160615235418.pdf)\n",
    "\n",
    "1시간만에 GAN(Generative Adversarial Network) 완전 정복하기 \n",
    "[최윤제](https://www.youtube.com/watch?v=odpjk7_tGY0)\n",
    "[local-slide](http://localhost:8888/notebooks/Dropbox/Paper/gan-170828035730.pdf)\n",
    "\n",
    "\n",
    "\n",
    "Generative Adversarial Networks [Hossein Azizpour](https://www.kth.se/social/files/59086d09f2765460c378ca73/GANs.pdf) [local-paper](http://localhost:8888/notebooks/Dropbox/Paper/GANs - Hossein Azizpour.pdf)\n",
    "\n",
    "\n",
    "Generative Adversarial Nets in TensorFlow (Part I) [blog](http://blog.evjang.com/2016/06/generative-adversarial-nets-in.html) [code](https://github.com/ericjang/genadv_tutorial/blob/master/genadv1.ipynb)\n",
    "\n",
    "Siraj Raval\n",
    "[GAN - Fresh Machine Learning #2](https://www.youtube.com/watch?v=deyOX6Mt_As&index=2&list=PL9hdKtbjIQ0wrszVsj3SWV-V2OAnMXZWa) [code](https://github.com/llSourcell/Generative-Adversarial-Network-Demo) [GAN (LIVE)](https://www.youtube.com/watch?v=0VPQHbMvGzg) [code](https://github.com/llSourcell/Generative_Adversarial_networks_LIVE/blob/master/EZGAN.ipynb) [How to Generate Video - Intro to Deep Learning #15](https://www.youtube.com/watch?v=-E2N1kQc8MM) [code](https://github.com/llSourcell/how_to_generate_video) [How to Convert Text to Images - Intro to Deep Learning #16](https://www.youtube.com/watch?v=gmvRStL_Dag) [code](https://github.com/llSourcell/how_to_convert_text_to_images) [Generate Music in TensorFlow](https://www.youtube.com/watch?v=ZE7qWXX05T0) [code](https://github.com/llSourcell/Music_Generator_Demo) [How to Generate Art - Intro to Deep Learning #8](https://www.youtube.com/watch?v=Oex0eWoU7AQ) [code](https://github.com/llSourcell/How-to-Generate-Art-Demo)\n",
    "\n",
    "Generative Models [OpenAI](https://blog.openai.com/generative-models/)\n",
    "\n",
    "<h2 id=\"threeapproachestogenerativemodels\">Three approaches to generative models</h2>\n",
    "\n",
    "<ul>\n",
    "<li><a href=\"http://arxiv.org/abs/1406.2661\">Generative Adversarial Networks (GANs)</a> pose the training process as a game between two separate networks: a generator network  and a second discriminative network that tries to classify samples as either coming from the true distribution <span class=\"MathJax_Preview\"></span><script type=\"math/tex\" id=\"MathJax-Element-10\">p(x)</script> or the model distribution <span class=\"MathJax_Preview\"></span><script type=\"math/tex\" id=\"MathJax-Element-11\">\\hat{p}(x)</script>. Every time the discriminator notices a difference between the two distributions the generator adjusts its parameters slightly to make it go away, until at the end (in theory) the generator exactly reproduces the true data distribution and the discriminator is guessing at random, unable to find a difference.</li>\n",
    "<li><a href=\"https://arxiv.org/abs/1312.6114\">Variational Autoencoders (VAEs)</a> allow us to formalize this problem in the framework of <a href=\"https://en.wikipedia.org/wiki/Graphical_model\">probabilistic graphical models</a> where we are maximizing a <a href=\"https://en.wikipedia.org/wiki/Variational_Bayesian_methods\">lower bound</a> on the log likelihood of the data.</li>\n",
    "<li>Autoregressive models such as <a href=\"http://arxiv.org/abs/1601.06759\">PixelRNN</a> instead train a network that models the conditional distribution of every individual pixel given previous pixels (to the left and to the top). This is similar to plugging the pixels of the image into a <a href=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\">char-rnn</a>, but the RNNs run both horizontally and vertically over the image instead of just a 1D sequence of characters.</li>\n",
    "</ul>\n",
    "\n",
    "Generalization and Equilibrium in GAN [Sanjeev Arora](https://www.youtube.com/watch?v=V7TliSCqOwI)\n",
    "\n",
    "Two-Sample Tests, Integral Probability Metrics, and GAN Objective [Dougal J. Sutherland](https://www.youtube.com/watch?v=Xpd6DL02C7Q)\n",
    "\n",
    "\n",
    "Autoencoder, GAN [nalsil](https://github.com/nalsil/TensorFlow-Tutorials/tree/master/04%20-%20Autoencoder%2C%20GAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src=\"img/Jensen–Shannon divergence.png\"/>\n",
    "\n",
    "<img src=\"img/Subderivative.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../../Data/MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting ../../../Data/MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../../Data/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../../Data/MNIST/t10k-labels-idx1-ubyte.gz\n",
      "Iter: 0, D loss: 1.098, G_loss: -0.0646\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-55821b8fb173>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mX_mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmb_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_loss_curr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mD_solver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msample_Z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmb_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG_loss_curr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mG_solver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msample_Z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmb_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mit\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-55821b8fb173>\u001b[0m in \u001b[0;36msample_Z\u001b[0;34m(m, n)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msample_Z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# https://github.com/wiseodd/generative-models/blob/master/GAN/vanilla_gan/gan_tensorflow.py\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "\n",
    "mb_size = 128\n",
    "Z_dim = 100\n",
    "\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)\n",
    "\n",
    "Z = tf.placeholder(tf.float32, shape=[None, Z_dim])\n",
    "\n",
    "G_W1 = tf.Variable(xavier_init([100, 128]))\n",
    "G_b1 = tf.Variable(tf.zeros(shape=[128]))\n",
    "\n",
    "G_W2 = tf.Variable(xavier_init([128, 784]))\n",
    "G_b2 = tf.Variable(tf.zeros(shape=[784]))\n",
    "\n",
    "theta_G = [G_W1, G_W2, G_b1, G_b2]\n",
    "\n",
    "def generator(z):\n",
    "    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)\n",
    "    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2\n",
    "    G_prob = tf.nn.sigmoid(G_log_prob)\n",
    "    return G_prob\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "\n",
    "D_W1 = tf.Variable(xavier_init([784, 128]))\n",
    "D_b1 = tf.Variable(tf.zeros(shape=[128]))\n",
    "\n",
    "D_W2 = tf.Variable(xavier_init([128, 1]))\n",
    "D_b2 = tf.Variable(tf.zeros(shape=[1]))\n",
    "\n",
    "theta_D = [D_W1, D_W2, D_b1, D_b2]\n",
    "\n",
    "def discriminator(x):\n",
    "    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)\n",
    "    D_logit = tf.matmul(D_h1, D_W2) + D_b2\n",
    "    D_prob = tf.nn.sigmoid(D_logit)\n",
    "    return D_prob, D_logit\n",
    "\n",
    "def sample_Z(m, n):\n",
    "    return np.random.uniform(-1., 1., size=[m, n])\n",
    "\n",
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "    return fig\n",
    "\n",
    "G_sample = generator(Z)\n",
    "D_real, D_logit_real = discriminator(X)\n",
    "D_fake, D_logit_fake = discriminator(G_sample)\n",
    "\n",
    "# D_loss = - tf.reduce_mean(tf.log(D_real)) - tf.reduce_mean(tf.log(1. - D_fake))\n",
    "# G_loss =                                    tf.reduce_mean(tf.log(1. - D_fake))\n",
    "D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, labels=tf.ones_like(D_logit_real)))\n",
    "D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.zeros_like(D_logit_fake)))\n",
    "D_loss = D_loss_real + D_loss_fake\n",
    "G_loss =             - D_loss_fake\n",
    "\n",
    "D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)\n",
    "G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)\n",
    "\n",
    "mnist = input_data.read_data_sets('../../../Data/MNIST', one_hot=True)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    if not os.path.exists('out/'):\n",
    "        os.makedirs('out/')\n",
    "\n",
    "    i = 0\n",
    "    for it in range(1000000):\n",
    "        X_mb, _ = mnist.train.next_batch(mb_size)\n",
    "        _, D_loss_curr = sess.run([D_solver, D_loss], feed_dict={X: X_mb, Z: sample_Z(mb_size, Z_dim)})\n",
    "        _, G_loss_curr = sess.run([G_solver, G_loss], feed_dict={Z: sample_Z(mb_size, Z_dim)})\n",
    "        if it % 1000 == 0:\n",
    "            print('Iter: {}, D loss: {:.4}, G_loss: {:.4}'.format(it, D_loss_curr, G_loss_curr))\n",
    "            samples = sess.run(G_sample, feed_dict={Z: sample_Z(16, Z_dim)})\n",
    "            fig = plot(samples)\n",
    "            plt.savefig('out/{}.png'.format(str(i).zfill(3)), bbox_inches='tight')\n",
    "            i += 1\n",
    "            plt.close(fig)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../../Data/MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting ../../../Data/MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../../Data/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../../Data/MNIST/t10k-labels-idx1-ubyte.gz\n",
      "Iter: 0, D loss: 1.49, G_loss: -0.067\n",
      "Iter: 1000, D loss: 0.02687, G_loss: -0.00905\n",
      "Iter: 2000, D loss: 0.0001796, G_loss: -7.123e-05\n",
      "Iter: 3000, D loss: 5.97e-05, G_loss: -1.724e-05\n",
      "Iter: 4000, D loss: 8.212e-05, G_loss: -1.345e-05\n",
      "Iter: 5000, D loss: 1.251e-05, G_loss: -6.708e-06\n",
      "Iter: 6000, D loss: 0.0001048, G_loss: -8.089e-06\n",
      "Iter: 7000, D loss: 7.236e-06, G_loss: -5.191e-06\n",
      "Iter: 8000, D loss: 1.08e-05, G_loss: -5.462e-06\n",
      "Iter: 9000, D loss: 5.908e-06, G_loss: -4.45e-06\n",
      "Iter: 10000, D loss: 1.26e-05, G_loss: -2.55e-06\n",
      "Iter: 11000, D loss: 2.481e-05, G_loss: -1.152e-07\n",
      "Iter: 12000, D loss: 3.737e-06, G_loss: -7.067e-08\n",
      "Iter: 13000, D loss: 3.746e-07, G_loss: -8.606e-08\n",
      "Iter: 14000, D loss: 3.698e-07, G_loss: -5.921e-08\n",
      "Iter: 15000, D loss: 1.749e-07, G_loss: -1.151e-07\n",
      "Iter: 16000, D loss: 1.445e-07, G_loss: -2.28e-08\n",
      "Iter: 17000, D loss: 3.917e-08, G_loss: -1.553e-08\n",
      "Iter: 18000, D loss: 3.022e-09, G_loss: -3.726e-09\n",
      "Iter: 19000, D loss: 5.111e-09, G_loss: -1.909e-09\n",
      "Iter: 20000, D loss: 5.525e-08, G_loss: -5.793e-09\n",
      "Iter: 21000, D loss: 2.23e-08, G_loss: -3.781e-08\n",
      "Iter: 22000, D loss: 3.259e-08, G_loss: -5.866e-08\n",
      "Iter: 23000, D loss: 1.448e-08, G_loss: -7.754e-10\n",
      "Iter: 24000, D loss: 5.516e-09, G_loss: -9.783e-10\n",
      "Iter: 25000, D loss: 4.523e-07, G_loss: -2.571e-07\n",
      "Iter: 26000, D loss: 2.523e-06, G_loss: -1.732e-08\n",
      "Iter: 27000, D loss: 9.093e-08, G_loss: -5.994e-09\n",
      "Iter: 28000, D loss: 5.711e-09, G_loss: -1.326e-11\n",
      "Iter: 29000, D loss: 6.33e-09, G_loss: -1.844e-09\n",
      "Iter: 30000, D loss: 4.11e-10, G_loss: -3.38e-10\n",
      "Iter: 31000, D loss: 9.434e-10, G_loss: -1.587e-08\n",
      "Iter: 32000, D loss: 3.808e-08, G_loss: -5.911e-11\n",
      "Iter: 33000, D loss: 1.597e-09, G_loss: -1.285e-11\n",
      "Iter: 34000, D loss: 3.223e-10, G_loss: -2.819e-12\n",
      "Iter: 35000, D loss: 1.333e-10, G_loss: -4.9e-10\n",
      "Iter: 36000, D loss: 5.55e-10, G_loss: -4.973e-13\n",
      "Iter: 37000, D loss: 2.195e-11, G_loss: -2.074e-13\n",
      "Iter: 38000, D loss: 1.911e-08, G_loss: -1.858e-12\n",
      "Iter: 39000, D loss: 1.728e-05, G_loss: -1.283e-05\n",
      "Iter: 40000, D loss: 4.495e-06, G_loss: -3.171e-08\n",
      "Iter: 41000, D loss: 3.448e-07, G_loss: -9.951e-12\n",
      "Iter: 42000, D loss: 1.48e-07, G_loss: -4.126e-06\n",
      "Iter: 43000, D loss: 2.851e-07, G_loss: -6.614e-10\n",
      "Iter: 44000, D loss: 5.13e-09, G_loss: -1.334e-09\n",
      "Iter: 45000, D loss: 4.492e-09, G_loss: -4.212e-11\n",
      "Iter: 46000, D loss: 6.602e-10, G_loss: -6.793e-11\n",
      "Iter: 47000, D loss: 1.71e-09, G_loss: -2.036e-11\n",
      "Iter: 48000, D loss: 6.375e-10, G_loss: -2.189e-11\n",
      "Iter: 49000, D loss: 2.651e-07, G_loss: -3.353e-10\n",
      "Iter: 50000, D loss: 1.622e-10, G_loss: -9.22e-11\n",
      "Iter: 51000, D loss: 1.349e-10, G_loss: -3.317e-11\n",
      "Iter: 52000, D loss: 7.045e-12, G_loss: -1.385e-10\n",
      "Iter: 53000, D loss: 3.205e-09, G_loss: -2.142e-12\n",
      "Iter: 54000, D loss: 6.621e-11, G_loss: -6.628e-11\n",
      "Iter: 55000, D loss: 2.419e-10, G_loss: -6.808e-11\n",
      "Iter: 56000, D loss: 0.0002361, G_loss: -0.0001011\n",
      "Iter: 57000, D loss: 6.923e-08, G_loss: -4.725e-07\n",
      "Iter: 58000, D loss: 9.322e-06, G_loss: -1.036e-06\n",
      "Iter: 59000, D loss: 2.541e-05, G_loss: -3.168e-07\n",
      "Iter: 60000, D loss: 3.008e-07, G_loss: -6.273e-07\n",
      "Iter: 61000, D loss: 3.767e-05, G_loss: -1.365e-06\n",
      "Iter: 62000, D loss: 7.537e-06, G_loss: -0.0001408\n",
      "Iter: 63000, D loss: 8.013e-07, G_loss: -1.176e-07\n",
      "Iter: 64000, D loss: 0.01378, G_loss: -1.178e-06\n",
      "Iter: 65000, D loss: 0.003202, G_loss: -0.009307\n",
      "Iter: 66000, D loss: 0.01573, G_loss: -0.006461\n",
      "Iter: 67000, D loss: 0.009394, G_loss: -0.002347\n",
      "Iter: 68000, D loss: 0.07898, G_loss: -0.0594\n",
      "Iter: 69000, D loss: 0.00233, G_loss: -0.007322\n",
      "Iter: 70000, D loss: 0.09007, G_loss: -0.01128\n",
      "Iter: 71000, D loss: 0.2142, G_loss: -0.0123\n",
      "Iter: 72000, D loss: 0.1133, G_loss: -0.0937\n",
      "Iter: 73000, D loss: 0.01105, G_loss: -0.008469\n",
      "Iter: 74000, D loss: 0.04873, G_loss: -0.009885\n",
      "Iter: 75000, D loss: 0.2264, G_loss: -0.02743\n",
      "Iter: 76000, D loss: 0.3088, G_loss: -0.1093\n",
      "Iter: 77000, D loss: 0.2795, G_loss: -0.1357\n",
      "Iter: 78000, D loss: 0.6579, G_loss: -0.1509\n",
      "Iter: 79000, D loss: 0.1762, G_loss: -0.06854\n",
      "Iter: 80000, D loss: 0.4582, G_loss: -0.167\n",
      "Iter: 81000, D loss: 0.959, G_loss: -0.465\n",
      "Iter: 82000, D loss: 0.9674, G_loss: -0.2677\n",
      "Iter: 83000, D loss: 0.6525, G_loss: -0.2956\n",
      "Iter: 84000, D loss: 0.4181, G_loss: -0.1529\n",
      "Iter: 85000, D loss: 0.8522, G_loss: -0.5774\n",
      "Iter: 86000, D loss: 0.6411, G_loss: -0.1954\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-858bb7179ecd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mX_mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmb_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_loss_curr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mD_solver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msample_Z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmb_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG_loss_curr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mG_solver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msample_Z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmb_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mit\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-858bb7179ecd>\u001b[0m in \u001b[0;36msample_Z\u001b[0;34m(m, n)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msample_Z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Non-Saturating Game\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "\n",
    "mb_size = 128\n",
    "Z_dim = 100\n",
    "\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)\n",
    "\n",
    "Z = tf.placeholder(tf.float32, shape=[None, 100])\n",
    "\n",
    "G_W1 = tf.Variable(xavier_init([100, 128]))\n",
    "G_b1 = tf.Variable(tf.zeros(shape=[128]))\n",
    "\n",
    "G_W2 = tf.Variable(xavier_init([128, 784]))\n",
    "G_b2 = tf.Variable(tf.zeros(shape=[784]))\n",
    "\n",
    "theta_G = [G_W1, G_W2, G_b1, G_b2]\n",
    "\n",
    "def generator(z):\n",
    "    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)\n",
    "    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2\n",
    "    G_prob = tf.nn.sigmoid(G_log_prob)\n",
    "    return G_prob\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "\n",
    "D_W1 = tf.Variable(xavier_init([784, 128]))\n",
    "D_b1 = tf.Variable(tf.zeros(shape=[128]))\n",
    "\n",
    "D_W2 = tf.Variable(xavier_init([128, 1]))\n",
    "D_b2 = tf.Variable(tf.zeros(shape=[1]))\n",
    "\n",
    "theta_D = [D_W1, D_W2, D_b1, D_b2]\n",
    "\n",
    "def discriminator(x):\n",
    "    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)\n",
    "    D_logit = tf.matmul(D_h1, D_W2) + D_b2\n",
    "    D_prob = tf.nn.sigmoid(D_logit)\n",
    "    return D_prob, D_logit\n",
    "\n",
    "def sample_Z(m, n):\n",
    "    return np.random.uniform(-1., 1., size=[m, n])\n",
    "\n",
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "    return fig\n",
    "\n",
    "G_sample = generator(Z)\n",
    "D_real, D_logit_real = discriminator(X)\n",
    "D_fake, D_logit_fake = discriminator(G_sample)\n",
    "\n",
    "# D_loss = - tf.reduce_mean(tf.log(D_real)) - tf.reduce_mean(tf.log(1. - D_fake))\n",
    "# G_loss =                                    tf.reduce_mean(tf.log(1. - D_fake))\n",
    "# Non-Saturating Game Implimentation \n",
    "# D_loss = - tf.reduce_mean(tf.log(D_real)) - tf.reduce_mean(tf.log(1. - D_fake))\n",
    "# G_loss =                                  - tf.reduce_mean(tf.log(D_fake))\n",
    "D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, labels=tf.ones_like(D_logit_real)))\n",
    "D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.zeros_like(D_logit_fake)))\n",
    "D_loss = D_loss_real + D_loss_fake\n",
    "G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.ones_like(D_logit_fake)))\n",
    "\n",
    "D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)\n",
    "G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)\n",
    "\n",
    "mnist = input_data.read_data_sets('../../../Data/MNIST', one_hot=True)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    if not os.path.exists('out/'):\n",
    "        os.makedirs('out/')\n",
    "\n",
    "    i = 0\n",
    "    for it in range(1000000):\n",
    "        X_mb, _ = mnist.train.next_batch(mb_size)\n",
    "        _, D_loss_curr = sess.run([D_solver, D_loss], feed_dict={X: X_mb, Z: sample_Z(mb_size, Z_dim)})\n",
    "        _, G_loss_curr = sess.run([G_solver, G_loss], feed_dict={Z: sample_Z(mb_size, Z_dim)})\n",
    "        if it % 1000 == 0:\n",
    "            print('Iter: {}, D loss: {:.4}, G_loss: {:.4}'.format(it, D_loss_curr, G_loss_curr))\n",
    "            samples = sess.run(G_sample, feed_dict={Z: sample_Z(16, Z_dim)})\n",
    "            fig = plot(samples)\n",
    "            plt.savefig('out/{}.png'.format(str(i).zfill(3)), bbox_inches='tight')\n",
    "            i += 1\n",
    "            plt.close(fig)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../../Data/MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting ../../../Data/MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../../Data/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../../Data/MNIST/t10k-labels-idx1-ubyte.gz\n",
      "Iter: 0, D loss: 1.475, G_loss: 2.428\n",
      "Iter: 1000, D loss: 0.004102, G_loss: 7.453\n",
      "Iter: 2000, D loss: 0.01213, G_loss: 6.836\n",
      "Iter: 3000, D loss: 0.05451, G_loss: 6.317\n",
      "Iter: 4000, D loss: 0.07819, G_loss: 5.822\n",
      "Iter: 5000, D loss: 0.1936, G_loss: 4.662\n",
      "Iter: 6000, D loss: 0.2009, G_loss: 5.081\n",
      "Iter: 7000, D loss: 0.3415, G_loss: 4.663\n",
      "Iter: 8000, D loss: 0.3717, G_loss: 4.625\n",
      "Iter: 9000, D loss: 0.3704, G_loss: 4.467\n",
      "Iter: 10000, D loss: 0.3441, G_loss: 5.103\n",
      "Iter: 11000, D loss: 0.6829, G_loss: 2.866\n",
      "Iter: 12000, D loss: 0.7625, G_loss: 3.688\n",
      "Iter: 13000, D loss: 0.4469, G_loss: 4.227\n",
      "Iter: 14000, D loss: 0.5369, G_loss: 3.965\n",
      "Iter: 15000, D loss: 0.9634, G_loss: 2.775\n",
      "Iter: 16000, D loss: 0.8356, G_loss: 3.255\n",
      "Iter: 17000, D loss: 0.6723, G_loss: 3.078\n",
      "Iter: 18000, D loss: 0.6191, G_loss: 2.89\n",
      "Iter: 19000, D loss: 0.8087, G_loss: 2.646\n",
      "Iter: 20000, D loss: 0.7143, G_loss: 2.757\n",
      "Iter: 21000, D loss: 0.6783, G_loss: 2.708\n",
      "Iter: 22000, D loss: 0.6414, G_loss: 2.724\n",
      "Iter: 23000, D loss: 0.7227, G_loss: 2.542\n",
      "Iter: 24000, D loss: 0.8939, G_loss: 2.194\n",
      "Iter: 25000, D loss: 0.7894, G_loss: 2.802\n",
      "Iter: 26000, D loss: 0.7407, G_loss: 2.899\n",
      "Iter: 27000, D loss: 0.632, G_loss: 2.326\n",
      "Iter: 28000, D loss: 0.7175, G_loss: 2.421\n",
      "Iter: 29000, D loss: 0.6028, G_loss: 2.724\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e41f255eff20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mX_mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmb_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_loss_curr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mD_solver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msample_Z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmb_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG_loss_curr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mG_solver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msample_Z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmb_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mit\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sungchul/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sungchul/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sungchul/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/sungchul/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sungchul/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Gradient Clipping \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "\n",
    "mb_size = 128\n",
    "Z_dim = 100\n",
    "\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)\n",
    "\n",
    "Z = tf.placeholder(tf.float32, shape=[None, 100])\n",
    "\n",
    "G_W1 = tf.Variable(xavier_init([100, 128]))\n",
    "G_b1 = tf.Variable(tf.zeros(shape=[128]))\n",
    "\n",
    "G_W2 = tf.Variable(xavier_init([128, 784]))\n",
    "G_b2 = tf.Variable(tf.zeros(shape=[784]))\n",
    "\n",
    "theta_G = [G_W1, G_W2, G_b1, G_b2]\n",
    "\n",
    "def generator(z):\n",
    "    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)\n",
    "    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2\n",
    "    G_prob = tf.nn.sigmoid(G_log_prob)\n",
    "    return G_prob\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "\n",
    "D_W1 = tf.Variable(xavier_init([784, 128]))\n",
    "D_b1 = tf.Variable(tf.zeros(shape=[128]))\n",
    "\n",
    "D_W2 = tf.Variable(xavier_init([128, 1]))\n",
    "D_b2 = tf.Variable(tf.zeros(shape=[1]))\n",
    "\n",
    "theta_D = [D_W1, D_W2, D_b1, D_b2]\n",
    "\n",
    "def discriminator(x):\n",
    "    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)\n",
    "    D_logit = tf.matmul(D_h1, D_W2) + D_b2\n",
    "    D_prob = tf.nn.sigmoid(D_logit)\n",
    "    return D_prob, D_logit\n",
    "\n",
    "def sample_Z(m, n):\n",
    "    return np.random.uniform(-1., 1., size=[m, n])\n",
    "\n",
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "    return fig\n",
    "\n",
    "def gradient_clip_by_value(grads_and_vars, clip_value_min, clip_value_max):\n",
    "    # Decomposition\n",
    "    grads = [grad for grad, _ in grads_and_vars]\n",
    "    vars = [var for _, var in grads_and_vars]\n",
    "\n",
    "    # Gradient Clipping\n",
    "    grads = [tf.clip_by_value(grad, clip_value_min, clip_value_max) for grad in grads]\n",
    "\n",
    "    # Reconstruct Grads and Vars.\n",
    "    grads_and_vars = zip(grads, vars)\n",
    "\n",
    "    return grads_and_vars\n",
    "\n",
    "G_sample = generator(Z)\n",
    "D_real, D_logit_real = discriminator(X)\n",
    "D_fake, D_logit_fake = discriminator(G_sample)\n",
    "\n",
    "# D_loss = - tf.reduce_mean(tf.log(D_real)) - tf.reduce_mean(tf.log(1. - D_fake))\n",
    "# G_loss =                                    tf.reduce_mean(tf.log(1. - D_fake))\n",
    "# Implimentation (CF: https://www.kth.se/social/files/59086d09f2765460c378ca73/GANs.pdf)\n",
    "# D_loss = - tf.reduce_mean(tf.log(D_real)) - tf.reduce_mean(tf.log(1. - D_fake))\n",
    "# G_loss =                                  - tf.reduce_mean(tf.log(D_fake))\n",
    "D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, labels=tf.ones_like(D_logit_real)))\n",
    "D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.zeros_like(D_logit_fake)))\n",
    "D_loss = D_loss_real + D_loss_fake\n",
    "G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.ones_like(D_logit_fake)))\n",
    "\n",
    "# Gradient Clipping\n",
    "clip_value_min = -1e-2\n",
    "clip_value_max = 1e-2\n",
    "\n",
    "opt = tf.train.AdamOptimizer()\n",
    "\n",
    "D_grads_and_vars = opt.compute_gradients(D_loss, var_list=theta_D)\n",
    "G_grads_and_vars = opt.compute_gradients(G_loss, var_list=theta_G)\n",
    "\n",
    "D_solver = opt.apply_gradients(gradient_clip_by_value(D_grads_and_vars, clip_value_min, clip_value_max))\n",
    "G_solver = opt.apply_gradients(gradient_clip_by_value(G_grads_and_vars, clip_value_min, clip_value_max))\n",
    "\n",
    "# D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)\n",
    "# G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)\n",
    "\n",
    "mnist = input_data.read_data_sets('../../../Data/MNIST', one_hot=True)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    if not os.path.exists('out/'):\n",
    "        os.makedirs('out/')\n",
    "\n",
    "    i = 0\n",
    "    for it in range(1000000):\n",
    "        X_mb, _ = mnist.train.next_batch(mb_size)\n",
    "        _, D_loss_curr = sess.run([D_solver, D_loss], feed_dict={X: X_mb, Z: sample_Z(mb_size, Z_dim)})\n",
    "        _, G_loss_curr = sess.run([G_solver, G_loss], feed_dict={Z: sample_Z(mb_size, Z_dim)})\n",
    "        if it % 1000 == 0:\n",
    "            print('Iter: {}, D loss: {:.4}, G_loss: {:.4}'.format(it, D_loss_curr, G_loss_curr))\n",
    "            samples = sess.run(G_sample, feed_dict={Z: sample_Z(16, Z_dim)})\n",
    "            fig = plot(samples)\n",
    "            plt.savefig('out/{}.png'.format(str(i).zfill(3)), bbox_inches='tight')\n",
    "            i += 1\n",
    "            plt.close(fig)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
